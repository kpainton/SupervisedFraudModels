{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "associate-penetration",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports 'n Such "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varying-alabama",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T03:27:15.118221Z",
     "start_time": "2021-05-01T03:27:15.112221Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fdr(model, x_df, y_df): \n",
    "    probs = pd.Series(model.predict_proba(x_df)[:,1]).sort_values(ascending=False)\n",
    "\n",
    "    numbads = y_df.sum()\n",
    "    topRows = int(round(len(probs)*0.03))\n",
    "    detected = y_df[probs.head(topRows).index].sum()\n",
    "    total = y_df.sum()\n",
    "\n",
    "    return((detected / total)*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedicated-australian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T03:27:15.358588Z",
     "start_time": "2021-05-01T03:27:15.354268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fdr_nn(model, x_df, y_df): \n",
    "    probs = model.predict(x_df)\n",
    "    probs = pd.DataFrame(probs, columns=['yup'])['yup'].sort_values(ascending=False)\n",
    "\n",
    "    numbads = y_df.sum()\n",
    "    topRows = int(round(len(probs)*0.03))\n",
    "    detected = y_df[probs.head(topRows).index].sum()\n",
    "    total = y_df.sum()\n",
    "\n",
    "    return((detected / total)*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "digital-wealth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T03:31:14.763427Z",
     "start_time": "2021-05-01T03:31:14.757369Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import xgboost as xgb \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-alert",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "sweet-memorabilia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:03:34.738528Z",
     "start_time": "2021-05-01T09:03:34.325521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('CreditCardFraudData_MostCurrent.csv', parse_dates = ['Date'])\n",
    "#mydata = pd.read_csv('CreditCardFraudData.csv', parse_dates = ['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-mailing",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "welcome-meaning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:03:36.374392Z",
     "start_time": "2021-05-01T09:03:36.340613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_test = mydata[mydata['Date'] <=  '2010-11-01']\n",
    "oot_X = mydata[mydata['Date'] >  '2010-11-01']\n",
    "\n",
    "oot_y = oot_X['Fraud']\n",
    "train_test_Y = train_test['Fraud']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "lovely-solomon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:03:47.824704Z",
     "start_time": "2021-05-01T09:03:47.786705Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Date</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum</th>\n",
       "      <th>Merchnum</th>\n",
       "      <th>Amount_Cardnum_Merch_description_sum_7</th>\n",
       "      <th>Amount_Merchnum_actual/median_365</th>\n",
       "      <th>Amount_Cardnum_sum_1</th>\n",
       "      <th>Amount_Cardnum_Merch_description_max_7</th>\n",
       "      <th>Amount_Cardnum_Merchnum_sum_30</th>\n",
       "      <th>Amount_Merchnum_max_1</th>\n",
       "      <th>Amount_Merchnum_max_3</th>\n",
       "      <th>Amount_Merchnum_max_14</th>\n",
       "      <th>Amount_Cardnum_Merch zip_median_180</th>\n",
       "      <th>Amount_Cardnum_Merch zip_max_30</th>\n",
       "      <th>Amount_Cardnum_sum_3</th>\n",
       "      <th>Amount_Cardnum_Industry_sum_0</th>\n",
       "      <th>Amount_Cardnum_median_30</th>\n",
       "      <th>Amount_Cardnum_Merch_description_sum_14</th>\n",
       "      <th>Amount_Cardnum_Merchnum_sum_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142190439</td>\n",
       "      <td>5.51E+12</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.620</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.620</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142183973</td>\n",
       "      <td>61003026333</td>\n",
       "      <td>31.42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.420</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.420</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142131721</td>\n",
       "      <td>4.50E+12</td>\n",
       "      <td>178.49</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.490</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.490</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142148452</td>\n",
       "      <td>5.51E+12</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.620</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.620</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142190439</td>\n",
       "      <td>5.51E+12</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.24</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.24</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.620</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>3.620</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83994</th>\n",
       "      <td>84486</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142121417</td>\n",
       "      <td>9.90E+12</td>\n",
       "      <td>40.01</td>\n",
       "      <td>1.285668</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "      <td>382.31</td>\n",
       "      <td>143.475</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "      <td>97.535</td>\n",
       "      <td>40.01</td>\n",
       "      <td>40.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83995</th>\n",
       "      <td>84487</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142268110</td>\n",
       "      <td>5.90E+12</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.00</td>\n",
       "      <td>606.32</td>\n",
       "      <td>3281.45</td>\n",
       "      <td>303.160</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83996</th>\n",
       "      <td>84488</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142124960</td>\n",
       "      <td>6.92E+12</td>\n",
       "      <td>243.62</td>\n",
       "      <td>1.635034</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.620</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "      <td>186.785</td>\n",
       "      <td>243.62</td>\n",
       "      <td>243.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83997</th>\n",
       "      <td>84489</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142182128</td>\n",
       "      <td>6.05E+12</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.300</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>115.400</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2325.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83998</th>\n",
       "      <td>84490</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5142280057</td>\n",
       "      <td>4.65E+11</td>\n",
       "      <td>173.75</td>\n",
       "      <td>0.715875</td>\n",
       "      <td>181.05</td>\n",
       "      <td>118.08</td>\n",
       "      <td>173.75</td>\n",
       "      <td>118.08</td>\n",
       "      <td>118.08</td>\n",
       "      <td>118.08</td>\n",
       "      <td>118.080</td>\n",
       "      <td>118.08</td>\n",
       "      <td>268.95</td>\n",
       "      <td>173.75</td>\n",
       "      <td>200.985</td>\n",
       "      <td>173.75</td>\n",
       "      <td>173.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83999 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Recnum       Date  Fraud     Cardnum     Merchnum  \\\n",
       "0           1 2010-01-01      0  5142190439     5.51E+12   \n",
       "1           2 2010-01-01      0  5142183973  61003026333   \n",
       "2           3 2010-01-01      0  5142131721     4.50E+12   \n",
       "3           4 2010-01-01      0  5142148452     5.51E+12   \n",
       "4           5 2010-01-01      0  5142190439     5.51E+12   \n",
       "...       ...        ...    ...         ...          ...   \n",
       "83994   84486 2010-11-01      0  5142121417     9.90E+12   \n",
       "83995   84487 2010-11-01      0  5142268110     5.90E+12   \n",
       "83996   84488 2010-11-01      0  5142124960     6.92E+12   \n",
       "83997   84489 2010-11-01      0  5142182128     6.05E+12   \n",
       "83998   84490 2010-11-01      0  5142280057     4.65E+11   \n",
       "\n",
       "       Amount_Cardnum_Merch_description_sum_7  \\\n",
       "0                                        3.62   \n",
       "1                                       31.42   \n",
       "2                                      178.49   \n",
       "3                                        3.62   \n",
       "4                                        7.24   \n",
       "...                                       ...   \n",
       "83994                                   40.01   \n",
       "83995                                   13.00   \n",
       "83996                                  243.62   \n",
       "83997                                 2325.30   \n",
       "83998                                  173.75   \n",
       "\n",
       "       Amount_Merchnum_actual/median_365  Amount_Cardnum_sum_1  \\\n",
       "0                               1.000000                  3.62   \n",
       "1                               1.000000                 31.42   \n",
       "2                               1.000000                178.49   \n",
       "3                               1.000000                  3.62   \n",
       "4                               1.000000                  7.24   \n",
       "...                                  ...                   ...   \n",
       "83994                           1.285668                 40.01   \n",
       "83995                           1.000000                 13.00   \n",
       "83996                           1.635034                243.62   \n",
       "83997                           1.000000               2325.30   \n",
       "83998                           0.715875                181.05   \n",
       "\n",
       "       Amount_Cardnum_Merch_description_max_7  Amount_Cardnum_Merchnum_sum_30  \\\n",
       "0                                        3.62                            3.62   \n",
       "1                                       31.42                           31.42   \n",
       "2                                      178.49                          178.49   \n",
       "3                                        3.62                            3.62   \n",
       "4                                        3.62                            7.24   \n",
       "...                                       ...                             ...   \n",
       "83994                                   40.01                           40.01   \n",
       "83995                                   13.00                           13.00   \n",
       "83996                                  243.62                          243.62   \n",
       "83997                                 2325.30                         2325.30   \n",
       "83998                                  118.08                          173.75   \n",
       "\n",
       "       Amount_Merchnum_max_1  Amount_Merchnum_max_3  Amount_Merchnum_max_14  \\\n",
       "0                       3.62                   3.62                    3.62   \n",
       "1                      31.42                  31.42                   31.42   \n",
       "2                     178.49                 178.49                  178.49   \n",
       "3                       3.62                   3.62                    3.62   \n",
       "4                       3.62                   3.62                    3.62   \n",
       "...                      ...                    ...                     ...   \n",
       "83994                  40.01                  40.01                  382.31   \n",
       "83995                  13.00                  13.00                   13.00   \n",
       "83996                 243.62                 243.62                  243.62   \n",
       "83997                2325.30                2325.30                 2325.30   \n",
       "83998                 118.08                 118.08                  118.08   \n",
       "\n",
       "       Amount_Cardnum_Merch zip_median_180  Amount_Cardnum_Merch zip_max_30  \\\n",
       "0                                    3.620                             3.62   \n",
       "1                                   31.420                            31.42   \n",
       "2                                  178.490                           178.49   \n",
       "3                                    3.620                             3.62   \n",
       "4                                    3.620                             3.62   \n",
       "...                                    ...                              ...   \n",
       "83994                              143.475                            40.01   \n",
       "83995                               13.000                            13.00   \n",
       "83996                              243.620                           243.62   \n",
       "83997                             2325.300                          2325.30   \n",
       "83998                              118.080                           118.08   \n",
       "\n",
       "       Amount_Cardnum_sum_3  Amount_Cardnum_Industry_sum_0  \\\n",
       "0                      3.62                           3.62   \n",
       "1                     31.42                          31.42   \n",
       "2                    178.49                         178.49   \n",
       "3                      3.62                           3.62   \n",
       "4                      7.24                           7.24   \n",
       "...                     ...                            ...   \n",
       "83994                 40.01                          40.01   \n",
       "83995                606.32                        3281.45   \n",
       "83996                243.62                         243.62   \n",
       "83997               2325.30                        2325.30   \n",
       "83998                268.95                         173.75   \n",
       "\n",
       "       Amount_Cardnum_median_30  Amount_Cardnum_Merch_description_sum_14  \\\n",
       "0                         3.620                                     3.62   \n",
       "1                        31.420                                    31.42   \n",
       "2                       178.490                                   178.49   \n",
       "3                         3.620                                     3.62   \n",
       "4                         3.620                                     7.24   \n",
       "...                         ...                                      ...   \n",
       "83994                    97.535                                    40.01   \n",
       "83995                   303.160                                    13.00   \n",
       "83996                   186.785                                   243.62   \n",
       "83997                   115.400                                  2325.30   \n",
       "83998                   200.985                                   173.75   \n",
       "\n",
       "       Amount_Cardnum_Merchnum_sum_1  \n",
       "0                               3.62  \n",
       "1                              31.42  \n",
       "2                             178.49  \n",
       "3                               3.62  \n",
       "4                               7.24  \n",
       "...                              ...  \n",
       "83994                          40.01  \n",
       "83995                          13.00  \n",
       "83996                         243.62  \n",
       "83997                        2325.30  \n",
       "83998                         173.75  \n",
       "\n",
       "[83999 rows x 20 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "instant-representative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:03:51.774622Z",
     "start_time": "2021-05-01T09:03:51.758504Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train_test.drop(columns=['Recnum', 'Date', 'Cardnum','Merchnum', 'Fraud'], inplace = True)\n",
    "oot_X.drop(columns=['Recnum', 'Date', 'Cardnum','Merchnum', 'Fraud'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-anime",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reflected-cradle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T03:32:35.906418Z",
     "start_time": "2021-05-01T03:32:35.900430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#metric tracking for debugging\n",
    "\n",
    "#NOTE three nums that get printed at end of run is FDR @3% in order: TRAIN TEST OOT\n",
    "\n",
    "model_fdr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-karen",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-tomato",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "animal-river",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T03:22:32.349656Z",
     "start_time": "2021-04-30T02:52:56.174321Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "LogisticRegression(C=0.1, class_weight={0: 1, 1: 10}, penalty='l1',\n",
      "                   solver='liblinear') 68.07575757575758 65.40909090909092 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "for i in range(10): \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = LogisticRegression(penalty='l1',solver = 'liblinear', C=.1, class_weight = {0:1, 1:10} )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-ceremony",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "pretty-trailer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T04:04:40.580934Z",
     "start_time": "2021-04-21T04:03:41.612215Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "LogisticRegression(C=1, solver='liblinear') 46.27272727272727 45.5 24.07821229050279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "    \n",
    "smote = SMOTE(sampling_strategy=.3)\n",
    "smote.fit_resample(train_test, train_test_Y)\n",
    "    \n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = LogisticRegression(penalty='l2',C=1,solver = 'liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-breach",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-promotion",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "polyphonic-collection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T20:48:33.858080Z",
     "start_time": "2021-05-01T20:48:06.876607Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "RandomForestClassifier(max_depth=15, min_samples_split=200, n_jobs=-1) 96.98484848484848 84.68181818181817 63.40782122905027\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "for i in range(10): \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "    X_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    X_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_X.reset_index(level = 0, drop = True, inplace = True)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = 100, criterion = 'gini', \n",
    "                                   max_depth =15, n_jobs=-1, verbose =0, min_samples_split = 200)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-ladder",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "liberal-agency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T20:50:07.336867Z",
     "start_time": "2021-05-01T20:49:57.125565Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "RandomForestClassifier(max_depth=10, min_samples_split=300, n_estimators=50,\n",
      "                       n_jobs=-1) 85.25757575757576 80.77272727272728 62.402234636871505\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "smote = SMOTE(sampling_strategy=.3)\n",
    "smote.fit_resample(train_test, train_test_Y)\n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators = 50, criterion = 'gini',\n",
    "                                  min_samples_split = 300, max_depth = 10, n_jobs=-1, verbose =0)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-corporation",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "young-outreach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T20:52:25.015819Z",
     "start_time": "2021-05-01T20:52:13.230291Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=300,\n",
      "                       n_estimators=50, n_jobs=-1) 93.25757575757575 82.68181818181817 56.14525139664804\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "RUS = RandomUnderSampler(sampling_strategy=.2)\n",
    "RUS.fit_resample(train_test, train_test_Y)\n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators = 50, criterion = 'entropy',\n",
    "                                  min_samples_split = 300, max_depth = 10, n_jobs=-1, verbose =0)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-notification",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-maximum",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "compatible-wireless",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T21:16:50.816184Z",
     "start_time": "2021-05-01T21:13:10.577275Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:13:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:13:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0\n",
      "[14:13:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:13:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[14:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[14:14:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:14:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[14:14:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:14:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[14:14:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:14:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[14:15:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:15:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[14:15:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:15:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[14:16:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:16:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[14:16:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:16:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.03, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=1000, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbose=0,\n",
      "              verbosity=None) 96.15151515151516 89.63636363636364 55.754189944134076\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "for i in range(10): \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators = 1000, learning_rate  = .03, subsample = 1,\n",
    "                                 max_depth = 3, n_jobs=-1, verbose = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-aurora",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "furnished-wichita",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T21:11:49.858023Z",
     "start_time": "2021-05-01T21:10:08.229055Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:10:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0\n",
      "[14:10:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[14:10:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[14:10:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[14:10:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[14:10:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[14:11:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[14:11:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[14:11:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[14:11:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=500, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None) 95.36363636363635 89.04545454545455 55.418994413407816\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "smote = SMOTE(sampling_strategy=.7)\n",
    "smote.fit_resample(train_test, train_test_Y)\n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators = 500, learning_rate  = .05,subsample = 1,\n",
    "                                 max_depth = 3, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-asthma",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "rocky-allergy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T21:18:49.057731Z",
     "start_time": "2021-05-01T21:18:28.592019Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0\n",
      "[14:18:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[14:18:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[14:18:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[14:18:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[14:18:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[14:18:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[14:18:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[14:18:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[14:18:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None) 87.98484848484847 83.86363636363636 52.569832402234645\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "RUS = RandomUnderSampler(sampling_strategy=.1)\n",
    "RUS.fit_resample(train_test, train_test_Y)\n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators = 100, learning_rate  = .1,subsample = 1,\n",
    "                                 max_depth = 3, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    x_val['train'].append(fdr(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-protocol",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "czech-cleaning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T23:10:17.492436Z",
     "start_time": "2021-04-22T23:10:17.483885Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-general",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "integrated-candy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:04:04.379094Z",
     "start_time": "2021-05-01T09:04:04.356090Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "train_test_nn = mms.fit_transform(train_test)\n",
    "oot_X_nn = mms.transform(oot_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "equivalent-attachment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T21:20:51.631726Z",
     "start_time": "2021-05-01T21:20:51.606707Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.17730162e-05, 2.46623178e-03, 1.17410573e-05, ...,\n",
       "        1.35295020e-04, 1.17730162e-05, 1.17730162e-05],\n",
       "       [1.02435025e-04, 2.46623178e-03, 1.02156956e-04, ...,\n",
       "        1.22549837e-03, 1.02435025e-04, 1.02435025e-04],\n",
       "       [5.82063141e-04, 2.46623178e-03, 5.80483078e-04, ...,\n",
       "        6.99298780e-03, 5.82063141e-04, 5.82063141e-04],\n",
       "       ...,\n",
       "       [7.94466617e-04, 4.03247744e-03, 7.92309965e-04, ...,\n",
       "        7.31828408e-03, 7.94466617e-04, 7.94466617e-04],\n",
       "       [7.58329001e-03, 2.46623178e-03, 7.56270448e-03, ...,\n",
       "        4.51885366e-03, 7.58329001e-03, 7.58329001e-03],\n",
       "       [5.66604943e-04, 1.76546576e-03, 5.88809146e-04, ...,\n",
       "        7.87515054e-03, 5.66604943e-04, 5.66604943e-04]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "destroyed-presentation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:06:39.648053Z",
     "start_time": "2021-05-01T21:53:58.789182Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 4s 639us/step - loss: 0.1695 - accuracy: 0.9890\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 598us/step - loss: 0.1612 - accuracy: 0.9895\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 593us/step - loss: 0.1526 - accuracy: 0.9901\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 618us/step - loss: 0.1704 - accuracy: 0.9890\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 593us/step - loss: 0.1609 - accuracy: 0.9896\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.1740 - accuracy: 0.9887\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 603us/step - loss: 0.1642 - accuracy: 0.9894\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.1494 - accuracy: 0.9903\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 596us/step - loss: 0.1594 - accuracy: 0.9897\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.1622 - accuracy: 0.9895\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 4s 599us/step - loss: 0.1598 - accuracy: 0.9896\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 4s 585us/step - loss: 0.1508 - accuracy: 0.9902\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 588us/step - loss: 0.1582 - accuracy: 0.9897\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 596us/step - loss: 0.1572 - accuracy: 0.9898\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 645us/step - loss: 0.1527 - accuracy: 0.9901\n",
      "0\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 4s 623us/step - loss: 0.1699 - accuracy: 0.9890\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 588us/step - loss: 0.1593 - accuracy: 0.9897\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.1606 - accuracy: 0.9896\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 649us/step - loss: 0.1678 - accuracy: 0.9891\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 726us/step - loss: 0.1594 - accuracy: 0.9897\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 753us/step - loss: 0.1627 - accuracy: 0.9895\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 688us/step - loss: 0.1702 - accuracy: 0.9890\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.1655 - accuracy: 0.9893\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 670us/step - loss: 0.1577 - accuracy: 0.9898\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 712us/step - loss: 0.1624 - accuracy: 0.9895\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 4s 699us/step - loss: 0.1684 - accuracy: 0.9891\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 4s 611us/step - loss: 0.1562 - accuracy: 0.9899\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 669us/step - loss: 0.1548 - accuracy: 0.9900\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 630us/step - loss: 0.1581 - accuracy: 0.9897\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 745us/step - loss: 0.1601 - accuracy: 0.9896\n",
      "1\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 6s 778us/step - loss: 0.0640 - accuracy: 0.9896\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 615us/step - loss: 0.0463 - accuracy: 0.9911\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 665us/step - loss: 0.0499 - accuracy: 0.9919\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 664us/step - loss: 0.0457 - accuracy: 0.9919\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 649us/step - loss: 0.0492 - accuracy: 0.9926\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 631us/step - loss: 0.0469 - accuracy: 0.9923\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 610us/step - loss: 0.0538 - accuracy: 0.9926\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 645us/step - loss: 0.0541 - accuracy: 0.9922\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 778us/step - loss: 0.0504 - accuracy: 0.9924\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 772us/step - loss: 0.0524 - accuracy: 0.9924\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 809us/step - loss: 0.0473 - accuracy: 0.9926\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 811us/step - loss: 0.0552 - accuracy: 0.9922\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 817us/step - loss: 0.0433 - accuracy: 0.9924\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 857us/step - loss: 0.0498 - accuracy: 0.9919\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 863us/step - loss: 0.0461 - accuracy: 0.9924\n",
      "2\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 6s 812us/step - loss: 0.0774 - accuracy: 0.9901\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 733us/step - loss: 0.0521 - accuracy: 0.9916\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 596us/step - loss: 0.0673 - accuracy: 0.9909\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 602us/step - loss: 0.0644 - accuracy: 0.9921\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 593us/step - loss: 0.0685 - accuracy: 0.9920\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 607us/step - loss: 0.0619 - accuracy: 0.9923\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 594us/step - loss: 0.0624 - accuracy: 0.9923\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 599us/step - loss: 0.0640 - accuracy: 0.9924\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.0662 - accuracy: 0.9930\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 626us/step - loss: 0.0667 - accuracy: 0.9922\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.99 - 4s 593us/step - loss: 0.0682 - accuracy: 0.9925\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 4s 605us/step - loss: 0.0664 - accuracy: 0.9924\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 590us/step - loss: 0.0655 - accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.0663 - accuracy: 0.9926\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 594us/step - loss: 0.0561 - accuracy: 0.9928\n",
      "3\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 678us/step - loss: 0.0807 - accuracy: 0.9892\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 648us/step - loss: 0.0652 - accuracy: 0.9905\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 670us/step - loss: 0.0617 - accuracy: 0.9914\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 616us/step - loss: 0.0548 - accuracy: 0.9925\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 593us/step - loss: 0.0513 - accuracy: 0.9918\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 602us/step - loss: 0.0543 - accuracy: 0.9921\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 594us/step - loss: 0.0649 - accuracy: 0.9922\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 591us/step - loss: 0.0613 - accuracy: 0.9921\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 616us/step - loss: 0.0602 - accuracy: 0.9925\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 620us/step - loss: 0.0468 - accuracy: 0.9920\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 4s 637us/step - loss: 0.0513 - accuracy: 0.9921\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 4s 656us/step - loss: 0.0710 - accuracy: 0.9921\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 605us/step - loss: 0.0557 - accuracy: 0.9921\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 632us/step - loss: 0.0603 - accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 706us/step - loss: 0.0517 - accuracy: 0.9926\n",
      "4\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 655us/step - loss: 0.1724 - accuracy: 0.9888\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 724us/step - loss: 0.1564 - accuracy: 0.9899\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 6s 894us/step - loss: 0.1540 - accuracy: 0.9900\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 6s 933us/step - loss: 0.1637 - accuracy: 0.9894\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 6s 915us/step - loss: 0.1614 - accuracy: 0.9895\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 6s 943us/step - loss: 0.1606 - accuracy: 0.9896\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 6s 980us/step - loss: 0.1626 - accuracy: 0.9895\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 6s 951us/step - loss: 0.1649 - accuracy: 0.9893\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.1592 - accuracy: 0.9897\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 6s 958us/step - loss: 0.1646 - accuracy: 0.9893\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 6s 892us/step - loss: 0.1632 - accuracy: 0.9894\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 6s 985us/step - loss: 0.1609 - accuracy: 0.9896\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 6s 915us/step - loss: 0.1594 - accuracy: 0.9897\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 872us/step - loss: 0.1716 - accuracy: 0.9889\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 810us/step - loss: 0.1595 - accuracy: 0.9897\n",
      "5\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 6s 837us/step - loss: 0.1645 - accuracy: 0.98930s - loss:\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 730us/step - loss: 0.1593 - accuracy: 0.9897\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 5s 717us/step - loss: 0.1554 - accuracy: 0.9899\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 752us/step - loss: 0.1604 - accuracy: 0.9896\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 734us/step - loss: 0.1713 - accuracy: 0.9889\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 745us/step - loss: 0.1672 - accuracy: 0.9892\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 767us/step - loss: 0.1503 - accuracy: 0.9903\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 769us/step - loss: 0.1688 - accuracy: 0.9891\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 794us/step - loss: 0.1609 - accuracy: 0.9896\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 806us/step - loss: 0.1573 - accuracy: 0.9898\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 832us/step - loss: 0.1662 - accuracy: 0.9892\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 866us/step - loss: 0.1564 - accuracy: 0.9899\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 871us/step - loss: 0.1681 - accuracy: 0.9891\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 6s 910us/step - loss: 0.1583 - accuracy: 0.9897\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 6s 944us/step - loss: 0.1533 - accuracy: 0.9901\n",
      "6\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0576 - accuracy: 0.9905\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0531 - accuracy: 0.9920\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0535 - accuracy: 0.9920\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0476 - accuracy: 0.9917\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0462 - accuracy: 0.9923\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0409 - accuracy: 0.9922\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 8s 1ms/step - loss: 0.0388 - accuracy: 0.9930\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0558 - accuracy: 0.9922\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0493 - accuracy: 0.9923\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0465 - accuracy: 0.9918: 0s - loss: 0.0465 - accuracy: 0.\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 6s 961us/step - loss: 0.0477 - accuracy: 0.9921\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 6s 928us/step - loss: 0.0408 - accuracy: 0.9931\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 6s 899us/step - loss: 0.0393 - accuracy: 0.9924\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 6s 885us/step - loss: 0.0436 - accuracy: 0.9926\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 6s 891us/step - loss: 0.0562 - accuracy: 0.9931\n",
      "7\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 7s 900us/step - loss: 0.1429 - accuracy: 0.99070s - loss: 0.1421 - accura\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 6s 911us/step - loss: 0.1600 - accuracy: 0.9896\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 5s 825us/step - loss: 0.1618 - accuracy: 0.9895\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 795us/step - loss: 0.1540 - accuracy: 0.9900\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 775us/step - loss: 0.1602 - accuracy: 0.9896\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 756us/step - loss: 0.1640 - accuracy: 0.9894\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 786us/step - loss: 0.1703 - accuracy: 0.9890\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 772us/step - loss: 0.1656 - accuracy: 0.9893\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 801us/step - loss: 0.1620 - accuracy: 0.9895\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 817us/step - loss: 0.1671 - accuracy: 0.9892\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 849us/step - loss: 0.1652 - accuracy: 0.9893\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 811us/step - loss: 0.1601 - accuracy: 0.9896\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 808us/step - loss: 0.1654 - accuracy: 0.9893\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 824us/step - loss: 0.1614 - accuracy: 0.9895\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 857us/step - loss: 0.1617 - accuracy: 0.9895\n",
      "8\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 6s 860us/step - loss: 0.0624 - accuracy: 0.9899\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 872us/step - loss: 0.0494 - accuracy: 0.9923\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 6s 921us/step - loss: 0.0613 - accuracy: 0.9917\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 6s 879us/step - loss: 0.0598 - accuracy: 0.9925\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 6s 901us/step - loss: 0.0668 - accuracy: 0.9923\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 6s 928us/step - loss: 0.0661 - accuracy: 0.9927\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 6s 968us/step - loss: 0.0622 - accuracy: 0.9923\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 6s 966us/step - loss: 0.0573 - accuracy: 0.9925\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 6s 1ms/step - loss: 0.0600 - accuracy: 0.9909\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0584 - accuracy: 0.9926\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0583 - accuracy: 0.9925\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0643 - accuracy: 0.9926\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 7s 1ms/step - loss: 0.0531 - accuracy: 0.9929\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 6s 914us/step - loss: 0.0610 - accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 822us/step - loss: 0.0691 - accuracy: 0.9920\n",
      "9\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001BB0618D160> 34.83333333333333 34.4090909090909 22.290502793296092\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "for i in range(10): \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test_nn, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10,input_dim =15, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=10)\n",
    "\n",
    "    x_val['train'].append(fdr_nn(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr_nn(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr_nn(model, oot_X_nn, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "oriented-excellence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:02:09.449360Z",
     "start_time": "2021-05-01T09:02:09.443363Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24012"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "variable-advisory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T09:01:27.905612Z",
     "start_time": "2021-05-01T09:01:27.867248Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount_Cardnum_Merch_description_sum_7</th>\n",
       "      <th>Amount_Merchnum_actual/median_365</th>\n",
       "      <th>Amount_Cardnum_sum_1</th>\n",
       "      <th>Amount_Cardnum_Merch_description_max_7</th>\n",
       "      <th>Amount_Cardnum_Merchnum_sum_30</th>\n",
       "      <th>Amount_Merchnum_max_1</th>\n",
       "      <th>Amount_Merchnum_max_3</th>\n",
       "      <th>Amount_Merchnum_max_14</th>\n",
       "      <th>Amount_Cardnum_Merch zip_median_180</th>\n",
       "      <th>Amount_Cardnum_Merch zip_max_30</th>\n",
       "      <th>Amount_Cardnum_sum_3</th>\n",
       "      <th>Amount_Cardnum_Industry_sum_0</th>\n",
       "      <th>Amount_Cardnum_median_30</th>\n",
       "      <th>Amount_Cardnum_Merch_description_sum_14</th>\n",
       "      <th>Amount_Cardnum_Merchnum_sum_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.64</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>122.61</td>\n",
       "      <td>41.76</td>\n",
       "      <td>27.88</td>\n",
       "      <td>180.16</td>\n",
       "      <td>180.16</td>\n",
       "      <td>509.88</td>\n",
       "      <td>34.820</td>\n",
       "      <td>41.76</td>\n",
       "      <td>122.61</td>\n",
       "      <td>27.88</td>\n",
       "      <td>152.27</td>\n",
       "      <td>69.64</td>\n",
       "      <td>27.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.37</td>\n",
       "      <td>0.963801</td>\n",
       "      <td>696.37</td>\n",
       "      <td>61.37</td>\n",
       "      <td>61.37</td>\n",
       "      <td>61.37</td>\n",
       "      <td>61.37</td>\n",
       "      <td>61.37</td>\n",
       "      <td>61.370</td>\n",
       "      <td>61.37</td>\n",
       "      <td>696.37</td>\n",
       "      <td>61.37</td>\n",
       "      <td>98.85</td>\n",
       "      <td>61.37</td>\n",
       "      <td>61.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>213.900</td>\n",
       "      <td>392.70</td>\n",
       "      <td>1249.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>224.50</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.69</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>119.66</td>\n",
       "      <td>51.69</td>\n",
       "      <td>51.69</td>\n",
       "      <td>51.69</td>\n",
       "      <td>51.69</td>\n",
       "      <td>51.69</td>\n",
       "      <td>51.690</td>\n",
       "      <td>51.69</td>\n",
       "      <td>119.66</td>\n",
       "      <td>51.69</td>\n",
       "      <td>41.65</td>\n",
       "      <td>51.69</td>\n",
       "      <td>51.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109.98</td>\n",
       "      <td>0.746133</td>\n",
       "      <td>109.98</td>\n",
       "      <td>109.98</td>\n",
       "      <td>2916.90</td>\n",
       "      <td>109.98</td>\n",
       "      <td>109.98</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>128.690</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>109.98</td>\n",
       "      <td>109.98</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2916.90</td>\n",
       "      <td>109.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64184</th>\n",
       "      <td>640.00</td>\n",
       "      <td>1.011858</td>\n",
       "      <td>5627.76</td>\n",
       "      <td>640.00</td>\n",
       "      <td>640.00</td>\n",
       "      <td>1458.00</td>\n",
       "      <td>1458.00</td>\n",
       "      <td>3234.00</td>\n",
       "      <td>640.000</td>\n",
       "      <td>640.00</td>\n",
       "      <td>5801.29</td>\n",
       "      <td>3422.17</td>\n",
       "      <td>3.62</td>\n",
       "      <td>640.00</td>\n",
       "      <td>640.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64185</th>\n",
       "      <td>2560.00</td>\n",
       "      <td>3.014129</td>\n",
       "      <td>7547.76</td>\n",
       "      <td>1920.00</td>\n",
       "      <td>2560.00</td>\n",
       "      <td>1920.00</td>\n",
       "      <td>1920.00</td>\n",
       "      <td>3234.00</td>\n",
       "      <td>1280.000</td>\n",
       "      <td>1920.00</td>\n",
       "      <td>7721.29</td>\n",
       "      <td>5342.17</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2560.00</td>\n",
       "      <td>2560.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64186</th>\n",
       "      <td>60.10</td>\n",
       "      <td>0.634854</td>\n",
       "      <td>1203.47</td>\n",
       "      <td>45.00</td>\n",
       "      <td>421.79</td>\n",
       "      <td>16.71</td>\n",
       "      <td>16.71</td>\n",
       "      <td>117.94</td>\n",
       "      <td>51.545</td>\n",
       "      <td>330.64</td>\n",
       "      <td>1314.76</td>\n",
       "      <td>60.10</td>\n",
       "      <td>111.29</td>\n",
       "      <td>60.10</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64187</th>\n",
       "      <td>450.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>420.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>450.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.00</td>\n",
       "      <td>420.00</td>\n",
       "      <td>420.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>450.00</td>\n",
       "      <td>420.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64188</th>\n",
       "      <td>480.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>450.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>480.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.00</td>\n",
       "      <td>450.00</td>\n",
       "      <td>450.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>480.00</td>\n",
       "      <td>450.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64189 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Amount_Cardnum_Merch_description_sum_7  \\\n",
       "0                                       69.64   \n",
       "1                                       61.37   \n",
       "2                                       40.00   \n",
       "3                                       51.69   \n",
       "4                                      109.98   \n",
       "...                                       ...   \n",
       "64184                                  640.00   \n",
       "64185                                 2560.00   \n",
       "64186                                   60.10   \n",
       "64187                                  450.00   \n",
       "64188                                  480.00   \n",
       "\n",
       "       Amount_Merchnum_actual/median_365  Amount_Cardnum_sum_1  \\\n",
       "0                               0.743665                122.61   \n",
       "1                               0.963801                696.37   \n",
       "2                               1.000000               1249.00   \n",
       "3                               1.000000                119.66   \n",
       "4                               0.746133                109.98   \n",
       "...                                  ...                   ...   \n",
       "64184                           1.011858               5627.76   \n",
       "64185                           3.014129               7547.76   \n",
       "64186                           0.634854               1203.47   \n",
       "64187                           1.000000                420.00   \n",
       "64188                           1.000000                450.00   \n",
       "\n",
       "       Amount_Cardnum_Merch_description_max_7  Amount_Cardnum_Merchnum_sum_30  \\\n",
       "0                                       41.76                           27.88   \n",
       "1                                       61.37                           61.37   \n",
       "2                                       40.00                           40.00   \n",
       "3                                       51.69                           51.69   \n",
       "4                                      109.98                         2916.90   \n",
       "...                                       ...                             ...   \n",
       "64184                                  640.00                          640.00   \n",
       "64185                                 1920.00                         2560.00   \n",
       "64186                                   45.00                          421.79   \n",
       "64187                                   30.00                          450.00   \n",
       "64188                                   30.00                          480.00   \n",
       "\n",
       "       Amount_Merchnum_max_1  Amount_Merchnum_max_3  Amount_Merchnum_max_14  \\\n",
       "0                     180.16                 180.16                  509.88   \n",
       "1                      61.37                  61.37                   61.37   \n",
       "2                      40.00                  40.00                   40.00   \n",
       "3                      51.69                  51.69                   51.69   \n",
       "4                     109.98                 109.98                 1500.00   \n",
       "...                      ...                    ...                     ...   \n",
       "64184                1458.00                1458.00                 3234.00   \n",
       "64185                1920.00                1920.00                 3234.00   \n",
       "64186                  16.71                  16.71                  117.94   \n",
       "64187                  30.00                  30.00                   30.00   \n",
       "64188                  30.00                  30.00                   30.00   \n",
       "\n",
       "       Amount_Cardnum_Merch zip_median_180  Amount_Cardnum_Merch zip_max_30  \\\n",
       "0                                   34.820                            41.76   \n",
       "1                                   61.370                            61.37   \n",
       "2                                  213.900                           392.70   \n",
       "3                                   51.690                            51.69   \n",
       "4                                  128.690                          1500.00   \n",
       "...                                    ...                              ...   \n",
       "64184                              640.000                           640.00   \n",
       "64185                             1280.000                          1920.00   \n",
       "64186                               51.545                           330.64   \n",
       "64187                               30.000                            30.00   \n",
       "64188                               30.000                            30.00   \n",
       "\n",
       "       Amount_Cardnum_sum_3  Amount_Cardnum_Industry_sum_0  \\\n",
       "0                    122.61                          27.88   \n",
       "1                    696.37                          61.37   \n",
       "2                   1249.00                          40.00   \n",
       "3                    119.66                          51.69   \n",
       "4                    109.98                         109.98   \n",
       "...                     ...                            ...   \n",
       "64184               5801.29                        3422.17   \n",
       "64185               7721.29                        5342.17   \n",
       "64186               1314.76                          60.10   \n",
       "64187                420.00                         420.00   \n",
       "64188                450.00                         450.00   \n",
       "\n",
       "       Amount_Cardnum_median_30  Amount_Cardnum_Merch_description_sum_14  \\\n",
       "0                        152.27                                    69.64   \n",
       "1                         98.85                                    61.37   \n",
       "2                        224.50                                    40.00   \n",
       "3                         41.65                                    51.69   \n",
       "4                        250.00                                  2916.90   \n",
       "...                         ...                                      ...   \n",
       "64184                      3.62                                   640.00   \n",
       "64185                      3.62                                  2560.00   \n",
       "64186                    111.29                                    60.10   \n",
       "64187                     30.00                                   450.00   \n",
       "64188                     30.00                                   480.00   \n",
       "\n",
       "       Amount_Cardnum_Merchnum_sum_1  \n",
       "0                              27.88  \n",
       "1                              61.37  \n",
       "2                              40.00  \n",
       "3                              51.69  \n",
       "4                             109.98  \n",
       "...                              ...  \n",
       "64184                         640.00  \n",
       "64185                        2560.00  \n",
       "64186                          15.10  \n",
       "64187                         420.00  \n",
       "64188                         450.00  \n",
       "\n",
       "[64189 rows x 15 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "foreign-steal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T08:59:51.765321Z",
     "start_time": "2021-05-01T08:59:51.758345Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1bb74cd6d30>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tender-chick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T08:00:55.108117Z",
     "start_time": "2021-05-01T08:00:55.103103Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [36.79653679653679,\n",
       "  34.1991341991342,\n",
       "  38.52813852813853,\n",
       "  39.82683982683983,\n",
       "  39.82683982683983,\n",
       "  38.52813852813853,\n",
       "  41.125541125541126,\n",
       "  35.4978354978355,\n",
       "  38.961038961038966,\n",
       "  34.63203463203463],\n",
       " 'test': [48.05194805194805,\n",
       "  40.25974025974026,\n",
       "  40.25974025974026,\n",
       "  40.25974025974026,\n",
       "  42.857142857142854,\n",
       "  48.05194805194805,\n",
       "  38.961038961038966,\n",
       "  35.064935064935064,\n",
       "  37.66233766233766,\n",
       "  38.961038961038966],\n",
       " 'oot': [67.37683089214381,\n",
       "  66.84420772303595,\n",
       "  67.11051930758988,\n",
       "  66.97736351531292,\n",
       "  67.37683089214381,\n",
       "  67.37683089214381,\n",
       "  67.37683089214381,\n",
       "  67.11051930758988,\n",
       "  67.11051930758988,\n",
       "  66.97736351531292]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(x_df)\n",
    "probs = pd.DataFrame(probs, columns=['yup'])['yup'].sort_values(ascending=False)\n",
    "\n",
    "numbads = y_df.sum()\n",
    "topRows = int(round(len(probs)*0.03))\n",
    "detected = y_df[probs.head(topRows).index].sum()\n",
    "total = y_df.sum()\n",
    "\n",
    "((detected / total)*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-publicity",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "educational-abraham",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:24:14.322684Z",
     "start_time": "2021-05-01T22:12:18.408671Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 4s 611us/step - loss: 0.1043 - accuracy: 0.9898\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 590us/step - loss: 0.0323 - accuracy: 0.9918\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 622us/step - loss: 0.0303 - accuracy: 0.9924\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 707us/step - loss: 0.0308 - accuracy: 0.9926\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 692us/step - loss: 0.0301 - accuracy: 0.9926\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 705us/step - loss: 0.0310 - accuracy: 0.9921\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 692us/step - loss: 0.0286 - accuracy: 0.9933\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 673us/step - loss: 0.0293 - accuracy: 0.9927\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 678us/step - loss: 0.0288 - accuracy: 0.9929\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 681us/step - loss: 0.0311 - accuracy: 0.9924\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 4s 699us/step - loss: 0.0275 - accuracy: 0.9930\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 731us/step - loss: 0.0266 - accuracy: 0.9935\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 674us/step - loss: 0.0275 - accuracy: 0.9929\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 686us/step - loss: 0.0274 - accuracy: 0.9934\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 698us/step - loss: 0.0283 - accuracy: 0.9930\n",
      "0\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 712us/step - loss: 0.1176 - accuracy: 0.9900\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 732us/step - loss: 0.0353 - accuracy: 0.9912\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 5s 740us/step - loss: 0.0350 - accuracy: 0.9919\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 742us/step - loss: 0.0329 - accuracy: 0.9925\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 745us/step - loss: 0.0310 - accuracy: 0.9927\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 750us/step - loss: 0.0345 - accuracy: 0.9921\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 767us/step - loss: 0.0304 - accuracy: 0.9929\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 784us/step - loss: 0.0316 - accuracy: 0.9924\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 780us/step - loss: 0.0320 - accuracy: 0.9926\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 776us/step - loss: 0.0323 - accuracy: 0.9924\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 763us/step - loss: 0.0305 - accuracy: 0.9923\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 738us/step - loss: 0.0295 - accuracy: 0.9927\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 729us/step - loss: 0.0329 - accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 716us/step - loss: 0.0286 - accuracy: 0.9932\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 711us/step - loss: 0.0297 - accuracy: 0.9924\n",
      "1\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 694us/step - loss: 0.1223 - accuracy: 0.9894\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 681us/step - loss: 0.0353 - accuracy: 0.9906\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 667us/step - loss: 0.0313 - accuracy: 0.9924\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 676us/step - loss: 0.0328 - accuracy: 0.9921\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 745us/step - loss: 0.0325 - accuracy: 0.9923\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 717us/step - loss: 0.0321 - accuracy: 0.9926\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 707us/step - loss: 0.0314 - accuracy: 0.9924\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 722us/step - loss: 0.0293 - accuracy: 0.9933\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 710us/step - loss: 0.0304 - accuracy: 0.9926\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 702us/step - loss: 0.0305 - accuracy: 0.9929\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 745us/step - loss: 0.0292 - accuracy: 0.9929\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 730us/step - loss: 0.0307 - accuracy: 0.9928\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 737us/step - loss: 0.0284 - accuracy: 0.9931\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 736us/step - loss: 0.0294 - accuracy: 0.9928\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 736us/step - loss: 0.0298 - accuracy: 0.9928\n",
      "2\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 6s 824us/step - loss: 0.1069 - accuracy: 0.9899\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 760us/step - loss: 0.0336 - accuracy: 0.9915\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 5s 787us/step - loss: 0.0312 - accuracy: 0.9925\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 799us/step - loss: 0.0310 - accuracy: 0.9923\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 805us/step - loss: 0.0281 - accuracy: 0.9934\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 851us/step - loss: 0.0317 - accuracy: 0.9925\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 832us/step - loss: 0.0304 - accuracy: 0.9928\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 835us/step - loss: 0.0299 - accuracy: 0.9929\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 785us/step - loss: 0.0294 - accuracy: 0.9933\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 756us/step - loss: 0.0289 - accuracy: 0.9928\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 786us/step - loss: 0.0301 - accuracy: 0.9927\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 749us/step - loss: 0.0280 - accuracy: 0.9929\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 704us/step - loss: 0.0301 - accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 735us/step - loss: 0.0278 - accuracy: 0.9931\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 699us/step - loss: 0.0281 - accuracy: 0.9933\n",
      "3\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 691us/step - loss: 0.1123 - accuracy: 0.9891\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 701us/step - loss: 0.0341 - accuracy: 0.9922\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 712us/step - loss: 0.0314 - accuracy: 0.9928\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 725us/step - loss: 0.0321 - accuracy: 0.9925\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 743us/step - loss: 0.0306 - accuracy: 0.9929\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 696us/step - loss: 0.0308 - accuracy: 0.9927\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 697us/step - loss: 0.0287 - accuracy: 0.9932\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 718us/step - loss: 0.0307 - accuracy: 0.9928\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 740us/step - loss: 0.0272 - accuracy: 0.9934\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 715us/step - loss: 0.0291 - accuracy: 0.9930\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 4s 695us/step - loss: 0.0288 - accuracy: 0.9930\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 4s 695us/step - loss: 0.0288 - accuracy: 0.9928\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 705us/step - loss: 0.0272 - accuracy: 0.9936\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 690us/step - loss: 0.0272 - accuracy: 0.9934\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 694us/step - loss: 0.0291 - accuracy: 0.9926\n",
      "4\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 698us/step - loss: 0.1216 - accuracy: 0.9891\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 700us/step - loss: 0.0376 - accuracy: 0.99150s -\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 710us/step - loss: 0.0326 - accuracy: 0.9923\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 731us/step - loss: 0.0306 - accuracy: 0.9929\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 758us/step - loss: 0.0320 - accuracy: 0.9924\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 724us/step - loss: 0.0314 - accuracy: 0.9924\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 735us/step - loss: 0.0286 - accuracy: 0.9932\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 788us/step - loss: 0.0291 - accuracy: 0.9931\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 814us/step - loss: 0.0304 - accuracy: 0.9927\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 815us/step - loss: 0.0299 - accuracy: 0.9929\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 834us/step - loss: 0.0295 - accuracy: 0.99260s - l\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 798us/step - loss: 0.0290 - accuracy: 0.9930\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 777us/step - loss: 0.0307 - accuracy: 0.9926\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 776us/step - loss: 0.0292 - accuracy: 0.9928\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 767us/step - loss: 0.0279 - accuracy: 0.9930\n",
      "5\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 729us/step - loss: 0.1270 - accuracy: 0.9859\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 719us/step - loss: 0.0336 - accuracy: 0.9921\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 711us/step - loss: 0.0336 - accuracy: 0.9923\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 731us/step - loss: 0.0333 - accuracy: 0.9922\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 690us/step - loss: 0.0345 - accuracy: 0.9922\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 4s 682us/step - loss: 0.0321 - accuracy: 0.9924\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 682us/step - loss: 0.0326 - accuracy: 0.9927\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 684us/step - loss: 0.0332 - accuracy: 0.9923\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 684us/step - loss: 0.0319 - accuracy: 0.9924\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 4s 684us/step - loss: 0.0305 - accuracy: 0.9928\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 4s 692us/step - loss: 0.0294 - accuracy: 0.9928\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 4s 696us/step - loss: 0.0339 - accuracy: 0.9917\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 690us/step - loss: 0.0308 - accuracy: 0.9925\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 718us/step - loss: 0.0285 - accuracy: 0.9931\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 695us/step - loss: 0.0301 - accuracy: 0.9924\n",
      "6\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 709us/step - loss: 0.1248 - accuracy: 0.9870\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 719us/step - loss: 0.0358 - accuracy: 0.99160s - loss: 0.0359 - \n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 5s 731us/step - loss: 0.0325 - accuracy: 0.9920\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 5s 749us/step - loss: 0.0337 - accuracy: 0.9921\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 774us/step - loss: 0.0326 - accuracy: 0.9922\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 779us/step - loss: 0.0325 - accuracy: 0.9923\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 809us/step - loss: 0.0323 - accuracy: 0.9923\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 810us/step - loss: 0.0314 - accuracy: 0.9925\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 844us/step - loss: 0.0305 - accuracy: 0.9927\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 6s 897us/step - loss: 0.0317 - accuracy: 0.9926\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 6s 950us/step - loss: 0.0300 - accuracy: 0.9931\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 862us/step - loss: 0.0288 - accuracy: 0.9927\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 5s 855us/step - loss: 0.0311 - accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 5s 808us/step - loss: 0.0315 - accuracy: 0.9926\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 790us/step - loss: 0.0296 - accuracy: 0.9932\n",
      "7\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 772us/step - loss: 0.1018 - accuracy: 0.9906\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 5s 786us/step - loss: 0.0320 - accuracy: 0.9924\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 5s 730us/step - loss: 0.0335 - accuracy: 0.9921\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 714us/step - loss: 0.0317 - accuracy: 0.9926\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 4s 700us/step - loss: 0.0322 - accuracy: 0.9925\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 716us/step - loss: 0.0312 - accuracy: 0.9929\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 4s 709us/step - loss: 0.0302 - accuracy: 0.9929\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 4s 710us/step - loss: 0.0321 - accuracy: 0.9926\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 4s 709us/step - loss: 0.0300 - accuracy: 0.9931\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 719us/step - loss: 0.0304 - accuracy: 0.9929\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 718us/step - loss: 0.0298 - accuracy: 0.9927\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 721us/step - loss: 0.0304 - accuracy: 0.9925\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 4s 680us/step - loss: 0.0290 - accuracy: 0.9933\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 4s 679us/step - loss: 0.0296 - accuracy: 0.9928\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 4s 689us/step - loss: 0.0286 - accuracy: 0.9932\n",
      "8\n",
      "Epoch 1/15\n",
      "6300/6300 [==============================] - 5s 705us/step - loss: 0.1277 - accuracy: 0.9859\n",
      "Epoch 2/15\n",
      "6300/6300 [==============================] - 4s 702us/step - loss: 0.0334 - accuracy: 0.9920\n",
      "Epoch 3/15\n",
      "6300/6300 [==============================] - 4s 701us/step - loss: 0.0329 - accuracy: 0.9918\n",
      "Epoch 4/15\n",
      "6300/6300 [==============================] - 4s 714us/step - loss: 0.0337 - accuracy: 0.9925\n",
      "Epoch 5/15\n",
      "6300/6300 [==============================] - 5s 722us/step - loss: 0.0309 - accuracy: 0.9929\n",
      "Epoch 6/15\n",
      "6300/6300 [==============================] - 5s 738us/step - loss: 0.0318 - accuracy: 0.9925\n",
      "Epoch 7/15\n",
      "6300/6300 [==============================] - 5s 753us/step - loss: 0.0315 - accuracy: 0.9927\n",
      "Epoch 8/15\n",
      "6300/6300 [==============================] - 5s 768us/step - loss: 0.0323 - accuracy: 0.9927\n",
      "Epoch 9/15\n",
      "6300/6300 [==============================] - 5s 791us/step - loss: 0.0314 - accuracy: 0.9928\n",
      "Epoch 10/15\n",
      "6300/6300 [==============================] - 5s 804us/step - loss: 0.0310 - accuracy: 0.9927\n",
      "Epoch 11/15\n",
      "6300/6300 [==============================] - 5s 818us/step - loss: 0.0276 - accuracy: 0.9936\n",
      "Epoch 12/15\n",
      "6300/6300 [==============================] - 5s 869us/step - loss: 0.0315 - accuracy: 0.9927\n",
      "Epoch 13/15\n",
      "6300/6300 [==============================] - 6s 881us/step - loss: 0.0299 - accuracy: 0.9930\n",
      "Epoch 14/15\n",
      "6300/6300 [==============================] - 6s 879us/step - loss: 0.0282 - accuracy: 0.9932\n",
      "Epoch 15/15\n",
      "6300/6300 [==============================] - 5s 843us/step - loss: 0.0305 - accuracy: 0.9926\n",
      "9\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001BB060A9F70> 66.78787878787878 66.95454545454547 39.050279329608934\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "smote = SMOTE(sampling_strategy=.3)\n",
    "smote.fit_resample(train_test_nn, train_test_Y)\n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test_nn, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim =15, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=10)\n",
    "\n",
    "\n",
    "    x_val['train'].append(fdr_nn(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr_nn(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr_nn(model, oot_X_nn, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-conviction",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "forty-amateur",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T08:19:33.603950Z",
     "start_time": "2021-04-21T08:14:36.463493Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 5s 768us/step - loss: 3.7892 - accuracy: 0.96620s - loss:\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 5s 760us/step - loss: 0.0627 - accuracy: 0.9895\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 5s 795us/step - loss: 0.0609 - accuracy: 0.9894\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 5s 804us/step - loss: 0.0758 - accuracy: 0.9883\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 5s 798us/step - loss: 0.0583 - accuracy: 0.9898\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 5s 830us/step - loss: 0.0650 - accuracy: 0.9892\n",
      "0\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 5s 808us/step - loss: 4.9329 - accuracy: 0.9052\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 5s 795us/step - loss: 0.0718 - accuracy: 0.9892\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 5s 797us/step - loss: 0.0612 - accuracy: 0.9901\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 5s 784us/step - loss: 0.0849 - accuracy: 0.9893\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 5s 786us/step - loss: 0.0782 - accuracy: 0.9891\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 5s 807us/step - loss: 0.0613 - accuracy: 0.9889\n",
      "1\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 6s 834us/step - loss: 20.9124 - accuracy: 0.9900\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 6s 1ms/step - loss: 0.0996 - accuracy: 0.9891\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 6s 937us/step - loss: 0.0544 - accuracy: 0.9903\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 5s 835us/step - loss: 0.0556 - accuracy: 0.9902\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 5s 838us/step - loss: 0.0754 - accuracy: 0.9898\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 5s 858us/step - loss: 0.0700 - accuracy: 0.9894\n",
      "2\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 6s 803us/step - loss: 5.0166 - accuracy: 0.9702\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 5s 806us/step - loss: 0.0581 - accuracy: 0.9902\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 5s 794us/step - loss: 0.0612 - accuracy: 0.9899\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 5s 790us/step - loss: 0.0630 - accuracy: 0.9903\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 5s 789us/step - loss: 0.0598 - accuracy: 0.9894\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 5s 825us/step - loss: 0.0595 - accuracy: 0.9890\n",
      "3\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 6s 843us/step - loss: 16.4325 - accuracy: 0.9180\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 5s 803us/step - loss: 0.0642 - accuracy: 0.9892\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 5s 803us/step - loss: 0.0590 - accuracy: 0.9892\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 5s 788us/step - loss: 0.0636 - accuracy: 0.9899\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 5s 788us/step - loss: 0.0593 - accuracy: 0.9896\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 5s 797us/step - loss: 0.0596 - accuracy: 0.9895\n",
      "4\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 6s 827us/step - loss: 2.2701 - accuracy: 0.9741\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 5s 824us/step - loss: 0.0751 - accuracy: 0.9894\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 5s 829us/step - loss: 0.0589 - accuracy: 0.9894\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 5s 862us/step - loss: 0.0747 - accuracy: 0.9898\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 6s 999us/step - loss: 0.0562 - accuracy: 0.9899\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 5s 863us/step - loss: 0.0614 - accuracy: 0.9901\n",
      "5\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 6s 870us/step - loss: 0.9425 - accuracy: 0.9839\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 4s 596us/step - loss: 0.0593 - accuracy: 0.9899\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 4s 583us/step - loss: 0.0719 - accuracy: 0.9900\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 4s 576us/step - loss: 0.0592 - accuracy: 0.9896\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 4s 585us/step - loss: 0.0586 - accuracy: 0.9902\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 4s 565us/step - loss: 0.0603 - accuracy: 0.9892\n",
      "6\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 4s 608us/step - loss: 5.5325 - accuracy: 0.9603\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 4s 604us/step - loss: 0.0994 - accuracy: 0.9883\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 4s 607us/step - loss: 0.0587 - accuracy: 0.9899\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 4s 631us/step - loss: 0.0557 - accuracy: 0.9902\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 4s 585us/step - loss: 0.0615 - accuracy: 0.98970s - loss: 0\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 4s 615us/step - loss: 0.0625 - accuracy: 0.9901\n",
      "7\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 4s 593us/step - loss: 5.8968 - accuracy: 0.9429\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 4s 614us/step - loss: 0.0672 - accuracy: 0.9895\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 4s 600us/step - loss: 0.0575 - accuracy: 0.9896\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 4s 577us/step - loss: 0.0623 - accuracy: 0.9889\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 4s 578us/step - loss: 0.0568 - accuracy: 0.9896\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 4s 569us/step - loss: 0.0581 - accuracy: 0.9894\n",
      "8\n",
      "Epoch 1/6\n",
      "6300/6300 [==============================] - 4s 597us/step - loss: 44.9836 - accuracy: 0.9045\n",
      "Epoch 2/6\n",
      "6300/6300 [==============================] - 4s 603us/step - loss: 0.0602 - accuracy: 0.9902\n",
      "Epoch 3/6\n",
      "6300/6300 [==============================] - 4s 592us/step - loss: 0.0545 - accuracy: 0.99030s\n",
      "Epoch 4/6\n",
      "6300/6300 [==============================] - 4s 575us/step - loss: 0.0934 - accuracy: 0.9892\n",
      "Epoch 5/6\n",
      "6300/6300 [==============================] - 4s 623us/step - loss: 0.0618 - accuracy: 0.9901\n",
      "Epoch 6/6\n",
      "6300/6300 [==============================] - 4s 569us/step - loss: 0.0582 - accuracy: 0.9894\n",
      "9\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x0000023731B71AF0> 3.696969696969697 3.5909090909090913 3.016759776536312\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'test':[],'oot':[]} \n",
    "\n",
    "RUS = RandomUnderSampler(sampling_strategy=.3)\n",
    "RUS.fit_resample(train_test, train_test_Y)\n",
    "\n",
    "for i in range(10): \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(train_test, train_test_Y,\n",
    "    test_size=0.25,\n",
    "    stratify=train_test_Y)\n",
    "\n",
    "    y_train.reset_index(level = 0, drop = True, inplace = True)\n",
    "    y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "    oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim =14, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=6, batch_size=10)\n",
    "\n",
    "    x_val['train'].append(fdr_nn(model, X_train, y_train)) \n",
    "    x_val['test'].append(fdr_nn(model, X_test, y_test)) \n",
    "    x_val['oot'].append(fdr_nn(model, oot_X, oot_y)) \n",
    "    print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['test']), np.mean(x_val['oot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "variable-belgium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T08:40:40.765195Z",
     "start_time": "2021-04-21T08:40:40.445473Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 473us/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-roller",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "nuclear-shell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:51:03.617654Z",
     "start_time": "2021-05-01T22:51:03.613653Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_final = train_test\n",
    "y_final = train_test_Y\n",
    "# oot_X\n",
    "# oot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "interesting-tunisia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:40:34.305773Z",
     "start_time": "2021-05-01T22:40:15.162123Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "RandomForestClassifier(max_depth=10, min_samples_split=300, n_estimators=50,\n",
      "                       n_jobs=-1) 84.55681818181819 62.17877094972067\n"
     ]
    }
   ],
   "source": [
    "x_val = {'train':[],'oot':[]} \n",
    "smote = SMOTE(sampling_strategy=.3)\n",
    "smote.fit_resample(train_test, train_test_Y)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test =\\\n",
    "#     train_test_split(train_test, train_test_Y,\n",
    "#     test_size=0.25,\n",
    "#     stratify=train_test_Y)\n",
    "\n",
    "#     y_train.reset_index(level = 0, drop 1= True, inplace = True)\n",
    "#     y_test.reset_index(level = 0, drop = True, inplace = True)\n",
    "oot_y.reset_index(level = 0, drop = True, inplace = True)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 50, criterion = 'gini',\n",
    "                              min_samples_split = 300, max_depth =10, n_jobs=-1, verbose =0)\n",
    "model.fit(X_final, y_final)\n",
    "\n",
    "x_val['train'].append(fdr(model, X_final, y_final)) \n",
    "x_val['oot'].append(fdr(model, oot_X, oot_y)) \n",
    "print(i)\n",
    "\n",
    "model_fdr.append([model, np.mean(x_val['train']), np.mean(x_val['oot'])])\n",
    "\n",
    "print(model, np.mean(x_val['train']), np.mean(x_val['oot']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "martial-accused",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:51:22.176308Z",
     "start_time": "2021-05-01T22:51:22.173308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_meas = oot_X\n",
    "y_meas = oot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "defined-carolina",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:51:24.282840Z",
     "start_time": "2021-05-01T22:51:24.145490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "probs = model.predict_proba(x_meas)\n",
    "probs = pd.DataFrame(probs, columns=['nope','predict_fraud_prob'])\n",
    "probs['predict'] = model.predict(x_meas)\n",
    "probs['actual_fraud_label'] = y_meas\n",
    "probs.drop(columns=['nope'], inplace=True)\n",
    "probs.sort_values('predict_fraud_prob', ascending =False, inplace=True)\n",
    "probs.to_csv('OOT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "convinced-medium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T22:48:37.782295Z",
     "start_time": "2021-05-01T22:48:37.776296Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_meas.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-syracuse",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
